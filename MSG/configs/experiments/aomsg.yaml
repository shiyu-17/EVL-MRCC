# sample style training script
dataset_path: './data/msg'
device: 1
pp_threshold: 0.3
object_threshold: 0.2

# coefficients for multitasking loss
loss_params:
  pr: 1.0
  obj: 1.0
  tcr: 0.0
  mean: 0.0

eval_split: Test
train_split: Training
# for train
output_dir: './exp-results/aomsg'
output_file: 'train'
wandb: True

# if resume training from a chkpt
resume: False
resume_path: './path/to/checkpoints/0+.pth'
# for eval
eval_output_dir: './exp-results/aomsg/2024-05-14_22-26-52' # this is specific to trained checkpoints
eval_chkpt: 29-step22470+.pth #null for no checkpoint
save_every: True # if save specific results for every video
eval_step: 100

learning_rate: 0.0002 # 5e-5
num_epochs: 30
warmup_epochs: 3
warmup: no # cos

num_workers: 8
train_bs: 384
bs_video: 6
eval_bs: 64 # debug test

log_every: 1
chkpt_every: 3000

obj_embedder:
  model: "dinov2-base" #"dinov2-small", "convnext-tiny-224", #'resnet50', # "dinov2_vits14", "dinov2_vitb14", "dinov2_vitl14", "dinov2_vitg14"
  weights: DEFAULT
  freeze: True
  output_type: feature # doesn't matter if embedding is cropped from image features BUT matters for sepMSG

place_embedder:
  model: "dinov2-base" #"convnext-tiny", #'resnet50', # "dinov2-base", "dinov2_vitb14", "dinov2_vitl14", "dinov2_vitg14"
  weights: DEFAULT
  freeze: True
  output_type: feature # mean, cls, feature

associator:
  model: "AoMSG-S-4" # "AoMSG-S-2"
  object_dim: 768 # FYI resnet18=512, dinov2-small=384, dinov2-base=768
  place_dim: 768
  output_dim: 1024


# loss terms
pr_loss: mse
obj_loss: bce
pos_weight: 10
pp_weight: 1